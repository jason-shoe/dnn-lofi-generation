File Explanations
lstm1.ipynb
lstm2.ipynb

lstm4.ipynb
    baseline for encoder-decoder architecture
    modification of ashwin.ipynb
lstm5.ipynb
    Loading the model and producing music
ashwin.ipynb
    Ashwin's notebook on April 3, 2016
    GRU, LSTM models
    Architecture is (batchsize, 1, 300) instead of (batchsize, 300, 1)

models/model1
    LSTM model with 15M parameters, 200 epochs of traiining at batch 32
    Training loss of 0.24, Validation loss of 0.6
    No regularization implemented
models/model2
    Same as above
    Training loss of 0.07
models/model3
    1000-100 input output split
    0.05 training and validation error after 7 training epochs
    min max scaler instead
    3 songs, stride of 25
model0-output.wav
    sample output from model0

helper.py
    Contains two functions
    Sequencing function and predicting function